
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


\input TEVC_style.sty


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
% \usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Bare Demo of IEEEtran.cls\\ for IEEE Journals}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space

% \thanks{M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

\author{Angus~Kenny$^a$, Dhananjay~Thiruvady$^b$, Davaatseren~Baatar$^c$, Andreas~T.~Ernst$^c$, Xiaodong~Li$^d$, Mohan~Krishnamoorthy$^e$, Gaurav~Singh$^f$%

\thanks{The authors are with: $^a$School of Engineering and Information Technology, University of New South Wales, Canberra ACT, Australia; $^b$School of Information Technology, Deakin University, Geelong VIC, Australia; $^c$School of Mathematics, Monash University, Clayton VIC, Australia; $^d$School of Science, RMIT University, Melbourne VIC, Australia; $^e$School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia QLD, Australia; $^f$BHP, Perth WA, Australia.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Open-pit mine scheduling is a challenging optimisation problem in the mining industry. It tries to create the best possible open-cut mine plan in order to maximise the 
net present value of an ore body. This leads to very large mixed integer programming problems that have a strong network structure which can be exploited to obtain a solution to the linear programming relaxation by repeatedly solving maximum flow problems. As these problems are too large and challenging to solve exactly, we have developed an efficient parallel  optimisation method to search for good heuristic solutions. The novel matheuristic proposed in this paper, called {\it Merge Search},  is able to combine a very large pool of solutions via variable aggregation, thereby using their best components to find higher quality solutions. The approach is built around a mixed integer programming formulation, where the formulation is made efficient via preprocessing. A key aspect of this study is to investigate an efficient parallelisation of Merge Search through distributed computing. We demonstrate empirically that this is the best performing method for the mine scheduling problem, finding better quality solutions for a number of problem instances available in the literature. The parallelisation also substantially improves the convergence characteristics of the method, even providing drastic improvements at the beginning of the search. Furthermore, we investigate the efficacy a parallel Branch \& Bound search. While the problems are too hard to be solved exactly, this improves upon the best known upper (relaxed) bounds for all problem instances. 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}\label{sec:intro}
% \cmnt[angus]{intro is too ``mining problem heavy''..\\some things to cover:
% \protect\begin{itemize}
% \protect\item large scale problems can be very complicated
% \protect\item decomposition algorithms important
% \protect\item local vs global search trade off (hybrid techniques)
% \protect\item general technique would be nice as most are domain knowledge dependent
% \protect\end{itemize}
% then after, introduce problems and say that they will demonstrate the versatility of the technique
% }
\IEEEPARstart{I}{n} the mining industry in Australia, open-pit mining is a problem of significant interest~\cite{Newman:2010}. The planning and production scheduling 
associated with mines can lead to significant cost savings and profits for the operators of the mines. In particular, determining the value of a mine 
and the areas of excavation in the shortest possible time-frame can lead to very large gains~\cite{Meagher2014}. 

Extracting and processing the ore from the pits is the main focus of open pit mining. Specifically, the order in which materials are extracted and processed 
can lead to significant profits and savings~\cite{Meagher2014}. In determining the order, there are several constraints that must be satisfied, including precedences between blocks and resource limits. Hence, the open-pit block scheduling problem is referred to as the {\it precedence constrained production scheduling problem} (PCPSP)~\cite{Bley:2010,
espinoza_minelib:_2012}. 

The PCPSP requires  identifying a schedule to extract blocks in a pit that maximises the net present value (NPV) of the blocks over time. Each block is associated 
with a positive value (profit) or a negative value (cost) and if mined, it is either processed or discarded (specified as destinations). The mining of 
blocks is subject to resource and precedence constraints. Resource limits apply to the amount of ore mined and processed during a period, while precedences 
between blocks exist due to pit slope constraints. That is, in order to reach certain blocks, other blocks on top of them need to be extracted and processed or 
discarded first.

The PCPSP can be formulated as a Mixed Integer Program (MIP). For real world instances, solving the MIP or even the linear programming (LP) relaxation of the problem is challenging due to the 
large number of variables and constraints. For example, the problem instances currently available in the literature 
can be very large with nearly 100,000 blocks and over a million precedence constraints. These need to be replicated over multiple time periods resulting 
in MIP formulations with an excess of 10 million constraints. 
 
Finding ways to solve large MIPs with reasonable computational resources has been given attention recently. The methods include decomposition approaches, hierarchical methods and MIP-based large neighbourhood search. Among MIP-based decompositions, Lagrangian relaxation \cite{fisher04}, column generation \cite{Wolsey1998} and Benders' decomposition \cite{Geoffrion1972} have been widely applied and proved very effective. Other approaches for efficiently solving MIPs is to identify aggregations of variables so that a problem of reduced size can be solved \cite{BOLAND2009, Litvinchev:2003, Rogers:1991}. Another class of methods are based on a large neighbourhood search (LNS) \cite{Ahuja:2002}. The main idea underlying these methods is to start with a feasible solution, identify a neighbourhood (possibly a very large), and find efficient ways to search the neighbourhood. When LNS is combined with MIPs, the resulting methods have proved very effective \cite{Ahuja:2002,Pisinger2010}. 

The main contribution of this study is a novel matheuristic algorithm, called {\it Merge Search}. It combines concepts from LNS, genetic algorithms \cite{Mitchell:1996} and the efficiency of solving MIPs. However, it is substantially different to any of these or other existing methods in two main aspects. First, the neighbourhood leading to the restricted MIP is obtained from an aggregation of variables in original model using a population of solutions (potentially very large populations). Using different input parameters (e.g. population size), the solving time of the restricted MIP can be systematically controlled. Second, Merge Search is particularly designed for parallel or distributed computing, thus allowing additional computing resources to be used effectively in tackling these challenging problems. Hence, a second contribution of this study is develop and investigate parallel implementations of Merge Search via the Message Passing Interface (MPI)~\cite{Gropp:1994}. A third contribution of this study is a Branch \& Bound method built around the LP relaxation of the problem. This is intended to show that even though the LP relaxation of the large MIPs can be solved relatively efficiently, the associated problems cannot be solved to optimality by a standard branch and bound approach. Despite this, we find that this method leads to identifying the best known upper bounds for benchmark instances of open pit mining. 

Recently, a method which uses a population of solutions to identify a search space for MIP model is construct, solve, merge and adapt (CMSA) \cite{Blum2016,BlumBlesa16,Blum16-2,Lewis2019,Thiruvady2019}. \cite{Blum2016} show that CMSA can be effectively applied to the minimum common string partition and minimum covering arborescence problems. \cite{BlumBlesa16} investigate CMSA for the repetition-free longest common subsequence problem with very good results can be obtained with this method and \cite{Blum16-2} show the same outcomes of CMSA for the unbalanced common string partition problem. \cite{Lewis2019} apply CMSA to happy colourings and show that this approach can find good solutions on hard problem instances where exact approaches struggle. \cite{Thiruvady2019} develop a parallel hyrbid of CMSA and ACO and show that it is effective on project scheduling with the aim of maximising the NPV.  

Merge Search was developed independently but uses similar ideas as that of CMSA at a high level. The crucial differences are in its ability to deal with extremely large populations of solutions and parallelisation. Like CMSA, the search iterative builds improving solutions to a problem by using the following steps: (a) maintain a population of feasible solutions, initially found through a heuristic (b) determine active variables in the MIP model from the population of solutions, (c) solve the resulting restricted MIP, thereby merging solutions, (d) use the solution information from the MIP to update the population, and (e) continue this process until some termination criteria is satisfied. Merge Search can be thought of as a generic matheuristic, combining integer programming and heuristic search,  but in this paper we only focus on its application to the PCPSP (Precedence Constrained Pit Scheduling Problem).

This document is organised as follows.  Section~\ref{sec:opbs} discusses the
details of the problem. This includes MIP formulations considered in this study and
equivalences to others published in the literature.  Next we introduce our new Merge Search heuristic and show how this can be applied to our problem in Section~\ref{sec:MS}. The details of how to apply this method to the PCPSP are described in Section~\ref{sec:meth}, including preprocessing, parallelisation and other implementational details that are important in obtaining good performance. Finally, we 
provide detailed computational results in Section~\ref{sec:expts_res} and show that we are able to produce both better feasible solutions and tighter bounds for most of the benchmarks that are available in the literature.

\section{Merge Search}\label{sec:MS}
For extremely large problems such as open pit mining, a method that is able to efficiently combine solutions (potentially obtained from different sources) can be beneficial. This is particularly important in a parallel or distributed framework where multiple threads or processes are independently finding improved solutions. In such cases we do not want to just take the best solution, thereby discarding any improvements made by all other processes, but to learn from each of them. For this purpose, the key procedure proposed in this paper and used throughout 
the distributed solver method is the {\it Merge Search}. The aim of this is to combine the best features of multiple solutions that may have been 
arrived at independently, often by starting from the previously best known solution. Merge Search achieves this by solving an integer program over the space of combinations of solutions. That is it breaks all of the solutions into fragments (a set of variables) and re-assembles a new solution from these fragments. 

\begin{algorithm}[htb!]
\caption{{\sf Merge Search Matheuristic}} \label{alg:MS}
\begin{algorithmic}[1]
  \Require A combinatorial optimisation problem with variables $x$: $\max f(x) : x\in {\cal F}$
  \Require Initial solution $x^0\in \cal F$
  \For{$k=1,2,\ldots$}
    \For{$j=1,2,\ldots,m$}
    \State Generate a solution $s^j$ in a large neighbourhood of $x^{k-1}$
    \label{step:nbhd}
      \EndFor
    \State Let  $S=\{s^1,\ldots,s^m\}\cup\{x^{k-1}\}$ be all such solutions
    \State Let ${\cal P}=\{P_1,\ldots,P_p\}$ be a partition of the variables into sets for which all solutions are constant:\label{step:P}
  $$ \bigcup_{P\in {\cal P}} P=\{1,\ldots,n\},\qquad P \cap Q=\emptyset\ \forall\ P\ne Q\in
  {\cal P},\quad \text{ and }\quad
  s_i = s_j \ \forall\ s\in S,\ P\in {\cal P},\ i,j\in P$$
  \State {\bf if} \ {$|\mathcal{P}| < K$} {\bf then} split subsets until $|\mathcal{P}|=K$ \label{step:RandSplit}
  \State Solve $x^k=\text{arg}\max f(x) : x\in {\cal F}\ \&\ x_i=x_j\ \forall
  i,j\in P\in{\cal P}$\label{step:merge}
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Description}
Merge Search is a general matheuristic that operates
as outlined in Algorithm~\ref{alg:MS}. It carries out the following steps:

\subsubsection*{Finding an initial solution}
In order to produce a population of solutions, an initial solution $x^0$ must first be produced.
Some problems, such as the PCPSP considered here, are so large and complex, that even producing a feasible solution is quite computationally expensive --- let alone one that is of guaranteed good quality. In contrast, other problems are structured such that producing a reasonable quality solution is not computationally expensive at all; however, often the methods of producing solutions to these problems are deterministic, creating issues with solution diversity. Although there are cases where finding a feasible solution is reasonably easy; in general, finding a feasible solution to a combinatorial problem is as hard as finding an optimal one~\cite{Papadimitriou1982}.

When considering Merge Search as a general framework for solving constrained optimisation problems, the method by which the initial solution is produced is not important. The ideal circumstance would be when there already exists a custom heuristic for constructing a solution to the problem. However, if no such heuristic exists, then because it can be shown (Lemma~\ref{thm:optimal}) that the random splitting aspect of Merge Search makes it theoretically capable of producing any solution in the search space, it is possible to start with a completely random (feasible) solution and still find the optimal solution --- if given enough time.

Of course, in practice, ``enough time'' can be infeasibly long for large search spaces, so a more intelligent method of producing an initial solution is required. Equation~\ref{eq:mip-init} gives a method of finding a feasible solution for problems which can be formulated as a mixed integer program (MIP) with $n$ variables and $m$ linear constraints.
%
\begin{align}\label{eq:mip-init}
              \Min \quad &\V{c}^T\V{x} + C^{max} \sum_{v_i \in \V{v}} v_i\\
              \ST  \quad  &\M{A}\V{x} \ge \V{b}-M\V{v}\nonumber\\
                   \quad & \V{x} \in \Bin^n, \quad \V{v} \in \Bin^m.\nonumber
\end{align}
%
Here, $\V{v} \in \Bin^m$ is a vector and $M$ is a constant sufficiently big enough such that $v_i = 1$ if constraint $i$ is violated and $C^{max} = \sum_{c_i \in \V{c}} |c_i|$ is a penalising constant that is added to the objective value every time a constraint is violated. The value of $C^{max}$ is calculated by summing the magnitude of every value in the $\V{c}$ vector, which means that it can be guaranteed that even the worst feasible solution will have a better objective value than the best infeasible one.

By solving this MIP model, a feasible solution to the problem is produced, which can then be used as an initial solution to generate a population\footnote{Actually, for sufficiently large \(C^{max}\), solving \eqref{eq:mip-init} is equivalent to solving the original problem (and will produce the same optimal solution) so, theoretically, there is no need to run the algorithm twice. In practice however, solving it in this way would be intractable for any problem that is large enough to be of interest, and so would not be a practical way of finding a feasible, initial solution. Any binary \(\V{x}\) can be used to produce a feasible initial solution for \eqref{eq:mip-init} by simply setting \(v_i=1\) iff the corresponding \(i\)th original constraint of \(\M{A} \V{x} \ge \V{b}\) is violated.}.

\subsubsection*{Neighbourhood search (Step~\ref{step:nbhd})}

Many solution merging meta-heuristics such as the construct, merge, solve and adapt (CMSA) heuristic~\cite{Blum2016} require a method that constructs solutions from scratch in order to produce a population to merge. However, for some large and complex problems, producing a feasible solution from scratch can be very computationally expensive. Merge search generates its population by defining a local-search operator for the problem and sampling the neighbourhood of a given solution; because it is generally easier to produce a new solution from an existing one, than it is to generate a solution from scratch. 

The local-search neighbourhood can be as simple, or as sophisticated, as is required. It can be a custom-built, problem specific, heuristic that always produces feasible solutions; or it can be a heuristic that simply generates random bit-strings. So long as there is at least one feasible solution in the population, it can be shown (Lemma~\ref{thm:improve}) that the merge operation will always produce a feasible solution and the random splitting heuristic ensures that any possible solution in the search space can be produced.

When considering Merge Search as a general meta-heuristic framework, it does not matter how the population is produced; some methods will produce populations that lead to more efficient searches, and some methods will produce populations that require a very long time to find the optimal solution. As with most hybrid meta-heuristic search techniques, it becomes about finding a good balance between how much computational effort is spent on exploration through, ensuring diversity of the population, and how much is spent on exploitation, through focusing on one particular area of the search space.

A generic neighbourhood search method can be defined for MIPs by fixing all but \(u\) variables (where \(u\) is a given fraction of the full set of decision variables) to the value of a given solution and solving this subproblem over the remaining \(u\) variables. Alternatively, a neighbourhood search can be performed generically using the machinery of Merge Search itself: define a random partition of variables, split that partition randomly and solve the subproblem induced by that partition. The number of neighbouring solutions $m$ is an algorithm parameter.

Once a population has been generated, it can be used to define a partition on the decision variables.

\subsubsection*{Defining the partition (Step~\ref{step:P})}

% can be implemented either by
%   starting with a single set $\{1,\ldots,n\}$ and iteratively partitioning it
%   where variables take on different values. Alternatively, we can define it
%   based on the unique set of solution values for $v\in\mathbb{R}^{|S|}$:
%   $P(v)=\{i\mid s_i = v_i\ \forall s\in S\}$ and $\mathcal{P}=\{P(v)\mid
%   P(v)\ne \emptyset\}$. At worst each set is a singleton, though in practice
%   we expect a quite small number of sets because (a) we are dealing with
%   combinatorial problems where most variables are either zero or one (or take
%   on a small number of values), and (b) the neighbourhood search tends to
%   generate a population of solutions with more similarity than just randomly
%   constructed solutions.

The simplest way to partition the decision variables for a problem with a population of solutions $S = \{s_1,\dots,s_m\}\cup \{x^{k-1}\}$ is to divide them into three groups: variables that \emph{always} take the value 0 across all solutions; variables that \emph{always} take the value 1 across all solutions; and variables that take \emph{either} 0 or 1 across all solutions. This partition can be used to produce a reduced subproblem with the variables that are in the first group fixed to 0; the variables that are in the second group fixed to 1; and the variables that are in the third group allowed to take either 0 or 1.

In this sense, Merge Search can be thought of as a generalisation of an optimised, multi-parent uniform crossover operator similar to that used in the genetic algorithm (GA). In the uniform crossover operator for GA~\cite{Luke2009}, each decision variable is considered in turn and its value selected from one of two parents with some probability. Because the value for every decision variable must be taken from either parent, no matter how many offspring are produced from these two parents the set of decision variables that are 1 for both parents will \emph{always} take the value 1, and the set of variables that are 0 for both parents will \emph{always} take the value 0. Effectively, these variables have been fixed and the only variables that are free to take either 0 or 1 are those that are not in either of these sets. This is exactly equivalent to performing a simple merge operation, without grouping, on a population consisting of two solutions; but where uniform crossover merely randomly samples the sub-space of solutions produced by the two parents, the merge operation searches that sub-space for the (locally) optimal offspring. Uniform crossover is also \emph{restricted} to a population of size two, whereas Merge Search generalises this idea to any arbitrary population size.

Using this na{\"\i}ve method of partitioning, with a very diverse population, when considering each variable in the $0/1$ region individually can result in very little reduction in the size of the subproblem produced. For this reason, a more sophisticated way of partitioning the decision variables is defined.

\begin{definition}\label{prop:part}
Let $x$ be the set of decision variables for a given problem, then the set $s_i \subseteq x$ is the set of decision variables that take the value 1 in a given solution $i$ and $S = \{s_1,\dots,s_m\}$ is a population of $m$ solutions. Now, a \bemph{merge partition} is the set ${\cal P} = \{P \mid \bigcap_{i=1}^{m} s^{b_i}_i,\ b \in \Bin^m\}$ %\mynotes{(are we allowed to say $\S{Z} \neq \emptyset$? isn't the empty-set an element of every set anyway, so subtracting it makes no sense..?)},
 % \{\S{Z}_1,\S{Z}_2,\dots,\S{Z}_k\}$ is defined such that each $\S{Z}_i$ is the (non-empty) intersection of $\S{S}_1^1$ or $\S{S}_1^0$ through to $\S{S}_m^1$ or $\S{S}_m^0$, 
where $s_i^1 = s_i$ and $s_i^0 = s \setminus s_i$.
\end{definition}

\begin{figure}[h!]
\centering
% \resizebox{.6\textwidth}{!}{
\includegraphics[width=0.45\textwidth]{group_part.tikz}
% }
\caption[Partitions formed by intersecting solutions]{Partitions formed by intersecting solutions.}
\label{fig:group_part}
\end{figure}

Figure~\ref{fig:group_part} illustrates all possible partitions that can be induced by a population of three solutions. The shaded circles indicate all of the variable assignments that are included in a particular solution. Here, partition $P_1=s^0_1 \cap s^0_2 \cap s^0_3$, which corresponds to the set of decision variables that take the value 0 across all solutions; partition $P_6=s^1_1 \cap s^1_2 \cap s^1_3$, which corresponds to the set of decision variables that take the value 1 across all solutions; and the other partitions correspond to sets of decision variables that, while they do not take the same value across all solutions, \emph{agree amongst themselves} across all solutions. For example, all decision variables in partition $P_5$ take a 0 in $s_2$ and a 1 in $s_1$ and $s_3$.
 
By partitioning the decision variables in this manner, it is reasonable to assume that --- given a big enough population size --- were a new solution to be generated, the majority of the decision variables in the new solution would agree with the other variables in their respective partitions. Therefore, the decision variables can be aggregated into groups in the reduced subproblem, with each partition being considered as an individual variable. 

The theoretical maximum number of partitions (and therefore, decision variables in the reduced subproblem) possible in a merge population of $m$ solutions is $2^m$. This theoretical maximum is only achieved when,
\begin{equation*}
\forall (s_i,s_j) \in S^2,\ s^b_i \cap s^c_j \neq \emptyset, \forall b,c \in \Bin; \label{sec:merge:part:bound}
\end{equation*}

however, typically not all solutions in a population will interact with all other solutions (i.e., there exist some pairs of solutions $(s_i,s_j) \in S^2$ such that $s^b_i \cap s^c_j \neq \emptyset, \forall b,c \in \Bin$), so $|{\cal P}| << 2^{|S|}$, in practice.

\subsubsection*{Random splitting (Step~\ref{step:RandSplit})}

When generating a population by sampling the neighbourhood around an initial solution, the size of the partition induced by this set of solutions is typically quite small. As the size of this partition directly affects the size of the reduced subproblem, it is useful to be able to control the size of partition to increase the size of the merge neighbourhood that is searched. One way to do this is through a process called \emph{random splitting}. 
%
\begin{definition}\label{def:split}
Given a set $\S{S}$, a \bemph{random split} is some heuristic process that produces two sets, $\S{S}_1$ and $\S{S}_2$, such that $\S{S}_1 \cup \S{S}_2 = \S{S}$ and $\S{S}_1 \cap \S{S}_2 = \emptyset$.
\end{definition}
%
This method of arbitrary splitting can be used to further partition the decision variables that have been aggregated, to allow the optimal solution to be produced. A proof of this is given in Lemma~\ref{thm:optimal}. 

Simply splitting the partitions arbitrarily is unlikely to produce a useful partitioning, let alone the optimal one; therefore, it is beneficial to use some heuristic strategy to do it --- this is especially true of large scale and highly constrained problems. For example, if $x_a$, $x_b$ and $x_c$ are decision variables in some partition $P_i$ and there are constraints in the problem model that say $x_a \leq x_b \leq x_c$, it does not make any sense to split $P_i$ such that $x_a,x_c \in {P_i}^\prime$ and $x_b \in {P_i}^{\prime\prime}$, as the values of the merge variables representing ${P_i}^\prime$ and ${P_i}^{\prime\prime}$ in a merged solution would have to be equal in order to remain feasible. In this case, a random splitting heuristic should be designed that takes these precedence relationships into account.

\medskip

There are many ways that a random splitting heuristic can be designed. The simplest is to generate a random bit string of length $n$ and add it to the population before the partition is defined (Figure~\ref{fig:random_split}). 

\begin{figure}[h]
\centering
\resizebox{.45\textwidth}{!}{
\includegraphics{split1.tikz}
}
\caption[Splitting partitions by adding a random bit string]
       {By adding the random bit string $s_M$ to the population before merging, each partition is arbitrarily split into two.}
\label{fig:random_split}
\end{figure}

In this figure, the ``natural'' partitions for the population $\{s_1,s_2,s_3\}$ are indicated by the blocks with shades of red, shades of blue and shades of green. By adding the random bit string $s_M$ to the population, each natural partition has been arbitrarily split into two, indicated by the light and dark colour shades.

While this method is the simplest, it does not take into account any of the constraints, or the implicit structure of the problem. This means that, for highly constrained problems, the partitioning produced by this method is likely to be no more effective than the natural partitioning induced by the population itself. Therefore, in practice, it is wise to design a heuristic splitting method with these considerations in mind; however, if no such method is possible, it is theoretically possible to produce the optimal solution by simply using random splitting alone --- when allowed enough time.

\medskip

If defining a partition on a set of solutions can be said to be analgous to the crossover operator for GA, then splitting the partition is analogous to its mutation operator. As has been already established, crossover only allows solutions to be sampled from the sub-space induced by the properties of the two parents, not the entire search space; the same is true for defining a partition on the decision variables as described in the previous section --- if $x_i = x_j$ across all solutions, any solution produced by merging in this way will also have $x_i = x_j$. In order to allow GA to produce any solution in the entire search space, a mutation operator is introduced; the simplest version of which selects a gene at random and flips its corresponding bit. This can be seen as a special case of the Merge Search heuristic, where a population consisting of a single solution and a single bit string with only one arbitrary 1 in it is merged. Here, the decision variables are partitioned into three groups: the group of variables that took the value 0 in the original solution; the group of variables that took the value 1 in the original solution; and the single variable to be ``mutated''. Again, as with the crossover analogy, whereas the mutation operator for GA merely samples the sub-space, Merge Search searches it to find the (locally) optimal choice.

By extending this idea further, it can also be shown that large neighbourhood search (LNS)~\cite{Pisinger2010}, or indeed any \emph{destroy-and-repair} heuristic, is a special case of Merge Search. To demonstrate this, a population is constructed that consists of a single solution $s$ and a set of unique bit string masks $\{M_1,M_2,\dots,M_m\}$, each with a single arbitrary bit flipped to 1, the rest of the bits are all zeroes. 

\begin{figure}[h]
\centering
\resizebox{.45\textwidth}{!}{
\includegraphics{lns-merge.tikz}
}
\caption[Demonstrtation that Merge Search is equivalent to LNS]
       {Each bit string mask separates its associated decision variable into a partition containing only that variable.}
\label{fig:lns-merge}
\end{figure}

Figure~\ref{fig:lns-merge} shows that when this population is merged, the decision variables are partitioned into $m+2$ groups: variables that took the value 0 in the original solution (red); variables that took the value 1 in the original solution (green); and $m$ partitions containing a single variable, associated with each of the bit string masks (shades of blue). If the ratio of $m$ to $n$ is sufficient, such that enough of the original solution structure is maintained, then all those variables that took 0 or 1 in the original solution will be effectively fixed to their respective values in the reduced subproblem, and all those variables in the $m$ singleton partitions are free to take either 0 or 1 in the solution to the reduced subproblem. This is equivalent to LNS, where a subset of the variables in a given solution is selected for ``destruction'' (i.e., removed from the solution) and the solution is ``repaired'' by solving the partial solution to (local) optimality. 

\subsubsection*{Solution merging (Step~\ref{step:merge})}
All of the steps leading up to this point have been working to construct a reduced subproblem, which can now be solved, using an exact solver or some other method, to produce a locally optimal solution for use in the next iteration of the process. In very general terms, the reduced subproblem takes the following form:
\begin{align}
              \Min \quad & f(\V{x})\label{eq:merge_group}\\
              \ST \quad  &\V{x} \in \S{F} \subseteq \Int_2^{|\V{x}|},\nonumber\\
              & x_i = x_j \quad \forall i,j \in P, P \in {\cal P}.\nonumber
\end{align}
Recall, $S$ is the generated population of solutions and ${\cal P} = \{P_1,P_2,\dots,P_p\}$ is the set of merge partitions produced by the population and the random splitting heuristic.

For problems with large numbers of decision variables, it can be practical to replace the set of problem variables $x$ with a vector of partition variables $z$. This will often create a certain amount of overhead in constructing the model for the reduced subproblem as constraints need to be transformed to be in terms of partition variables instead of decision variables --- and then again, when the produced solution must be re-expressed in terms of the problem variables. However, this usually results in the model taking up much less space in memory.

Finding the globally optimal solution to the reduced subproblem will give a locally optimal solution to the master problem Figure~\ref{fig:merge-solve} gives an illustration of this process.

\begin{figure*}[h]
  \centering
  \subfloat[Optimal solution $x^*$\label{fig:merge-opt}]{\includegraphics[width = 0.23\textwidth]{merge-opt2.eps}}\quad 
  \subfloat[Merge partitions ${\cal P}$\label{fig:merge-nhood}]{\includegraphics[width = 0.23\textwidth]{merge-nhood_red.eps}}\quad%\\ 
  \subfloat[Sub-problem solution $z^*$\label{fig:merge-sol}]{\includegraphics[width = 0.23\textwidth]{merge-sol.eps}}\quad
  \subfloat[Merged $f(x^k) \leq f(x^*)$\label{fig:merge-approx}]{\includegraphics[width = 0.23\textwidth]{merge-approx.eps}}
  \caption[Solving the reduced subproblem gives an approximate solution to the master problem]{Solving the reduced subproblem induced by the merge partitions ${\cal P}$ to optimality, gives an approximate solution to the master problem.} 
    \label{fig:merge-solve}
\end{figure*}

The partition on the decision variables representing the optimal solution $x^* \in {\cal F}$ is shown in blue in Figure~\ref{fig:merge-opt}. Figure~\ref{fig:merge-nhood} shows the merge partitions produced by merging the population $S$ and applying the random splitting heuristic. Here, the initial solution $x^{k-1}$ is the straight red partition boundary and each neighbouring solution in the population is represented by two triangular regions protruding from either side of this boundary. These regions represent the decision variables that take different values in the neighbouring solution, with respect to the initial --- regions on the left of the boundary indicate variables that have changed from 0 to 1 in the new solution, regions on the right indicate those that have changed from 1 to 0. The population is merged and split to produce the reduced subproblem which is expressed in terms of a set of partition variables $z$. This reduced subproblem is then solved using an exact solver or some other method. Figure~\ref{fig:merge-sol} shows that the optimal solution to the subproblem $z^*$ is a partition constructed from a subset of the boundaries of the merge partitions. Finally, $z^*$ is mapped back to a solution to the master problem $x^k$ (Figure~\ref{fig:merge-sol}), an approximation of $x^*$. If the stopping criteria is not yet met, then the merged solution $x^k$ is used as the initial solution for the next iteration.

Provided there is at least one feasible solution in the population at the time of merging, the merged solution $x^k$ will always be feasible. As the merge partitions are formed by the intersections of all the members in the population $S$ (and then randomly split), any solution $s_i \in S$ can be reconstructed by simply including all partitions that are associated with that solution. This is illustrated in Figure~\ref{fig:group_part} above, where $s_1$ can be constructed by the union of partitions $P_2 \cup P_3 \cup P_5 \cup P_6$. If $s_i$ is feasible, then the final merge operation is able to produce $s_i$ as $x^k$; meaning that, so long as the initial solution $x^{k-1}$ is feasible, $x^{k-1}$ will be a lower bound on the solution produced by merging and $f(x^{k-1}) \leq f(x^k) \leq f(x^*)$, even if the entire rest of the population consists of randomly generated bit strings, representing infeasible solutions.

% \begin{itemize}
% \item Neighbourhood Search (Step~\ref{step:nbhd}): This generates a population
%   of solutions for the merge step. While any heuristic method could be used
%   here, it will be assumed that it produces a randomised solution (so that the
%   solutions are nearly always different) and that the neighbourhood is sufficiently
%   large that the solutions produced will differ from the original $x^{k-1}$ in
%   several variables. The number of neighbouring solutions $m$ is an algorithm
%   parameter.
% \item Partitioning (Step~\ref{step:P}) can be implemented either by
%   starting with a single set $\{1,\ldots,n\}$ and iteratively partitioning it
%   where variables take on different values. Alternatively, we can define it
%   based on the unique set of solution values for $v\in\mathbb{R}^{|S|}$:
%   $P(v)=\{i\mid s_i = v_i\ \forall s\in S\}$ and $\mathcal{P}=\{P(v)\mid
%   P(v)\ne \emptyset\}$. At worst each set is a singleton, though in practice
%   we expect a quite small number of sets because (a) we are dealing with
%   combinatorial problems where most variables are either zero or one (or take
%   on a small number of values), and (b) the neighbourhood search tends to
%   generate a population of solutions with more similarity than just randomly
%   constructed solutions.
% \item Random Splitting (Step~\ref{step:RandSplit}) allows more control
%   over the size of $\mathcal{P}$ and hence the size of the problem solved in
%   the next step as typically $|\mathcal{P}|$ is quite small. Hence, this step creates 
%   a larger neighbourhood by carrying out random splitting of subsets. In fact it is
%   possible to run an extreme form of this algorithm with $m=0$ so that only
%   the random splitting in this step is used to define a neighbourhood. 
%   The choice of how to split subsets (perhaps with some biased randomisation) 
%   should be done in a problem dependent manner, as the choice of neighbourhood 
%   generated in this manner will clearly affect the effectiveness of the overall 
%   algorithm.
% \item Merging of solutions occurs in Step~\ref{step:merge}. It should be noted that this is
%   essentially a variant of the original problem but with significantly fewer
%   variables (one variable per $P\in\mathcal{P}$, $|\mathcal{P}|=K\ll n$). This
%   makes the step particularly amenable to use with integer or constraint
%   programming, where general purpose solvers generally cope well with small to
%   medium sized problems but solve times deteriorate rapidly as the problem
%   size increases. 
% \end{itemize}

\subsection{Properties of Merge Search}
There are some simple but important properties that follow directly from the way this matheuristic has been defined.

\begin{lemma}\label{thm:improve}
  The Merge Step~\ref{step:merge} produces a solution that is at least as good
  as any of the neighbours in $S$: $f(x^k) \ge f(s)\ \forall s\in S$
\end{lemma}
\begin{proof}
  By construction any solution $s\in S$ satisfies $s\in \mathcal{F}$ and
  $s_i=s_j$ $\forall i,j\in P\in\mathcal{P}$ and so is feasible for the
  optimisation problem in Step~\ref{step:merge}. Hence, $f(x^k)\ge f(s)$.
\end{proof}

\begin{corollary}\label{thm:hill-climb}
  Merge Search is a hill-climbing method that produces a non-decreasing sequence
 of solution values: $f(x^0)\le f(x^1)\le f(x^2)\le \ldots$
\end{corollary}

Hence, for diversification, the method relies entirely on the neighbourhood
search in Step~\ref{step:nbhd} of the algorithm and the randomised splitting in
Step~\ref{step:RandSplit}. This is not a problem for large instances such as
this, where simply finding a very good local minimum is already a very
challenging task. Furthermore, the following property holds:

\begin{lemma}\label{thm:optimal}
  For pure binary problems where all variables are $0-1$, if random splitting of subsets is used in Step~\ref{step:RandSplit} (allowing any possible split with some non-zero probability) with $K\ge 2^{m+2}$ then Algorithm~\ref{alg:MS} is guaranteed to converge to the optimal solution as the number of iterations approaches infinity. 
\end{lemma}
\begin{proof}
  By assumption there exists an optimal solution in $\cal F$, due to the existence of $x^0\in\cal F$ and boundedness of $\cal F$ when all variables are binary. Let $s^*$ be any optimal solution. The first thing to note is that if a partition $\mathcal{P}^*$ satisfies $s^*_i=s^*_j\ \forall P\in \mathcal{P}^*,\ i,j\in P$ then the Merge problem in Step~\ref{step:merge} yields and optimal solution. 
  Such a partition could be generated, from any arbitrary partition $\cal P$ by splitting each $P\in\cal P$ into $P_0=P\cap \{i\mid s^*_i=0\}$ and $P_1=P\cap \{i\mid s^*_i=1\}$. This splitting at most doubles the number of elements in the partition. \\
  Now the same argument can be used to show that $|\mathcal{P}|$ as produced in Step~\ref{step:P} has at most $2^{m+1}$ elements (corresponding to splitting based on solutions $s^1,\ldots,s^m$ and $x^{k-1}$). Hence, we only need $K$ to be at least $2^{m+2}$ to allow some chance of generating the required partition in any step. Hence, as the number of iterations of Algorithm~\ref{alg:MS} goes to infinity the chance of \emph{not} producing an optimal solution goes to zero. And of course, based on Corollary~\ref{thm:hill-climb} once an optimal solution has been found, the algorithm will not depart from this.
\end{proof}
While convergence to the optimal solution is of course extremely unlikely for practical sized instances, the lemma shows that it is at least theoretically possible. Furthermore, while $2^{m+2}$ appears quite large, the only real requirement is that $K$ is sufficiently large to allow each element of $\cal P$ to be split once, with $|\mathcal{P}|$ typically much smaller than $2^{m+1}$ in practice. Hence, while Merge Search is a hill-climbing method, it is at least in principle possible for the method to reach a global optimum from any starting point.


\subsection{Comparison with CMSA}
As mentioned previously, it may be noted here that this meta-heuristic has some similarity to the
recently published CMSA heuristic by~\cite{Blum2016}. Starting with an empty subinstance of the problem $\mathcal{C}^\prime$, solutions are probabilistically generated in this method, from scratch, and their components added to $\mathcal{C}^\prime$. This reduced subinstance is then solved using an exact solver and a so-called ``ageing'' mechanism is used to remove components from $\mathcal{C}^\prime$ that have not been useful in the preceding iterations. 

Although there are some similarities between Merge Search and CMSA, there are two areas where Merge Search diverges significantly from it. These are:
\begin{itemize}
\item the generation of candidate solutions to be merged; and
\item the aggregation of decision variables in the reduced subproblem.
\end{itemize}
The CMSA subproblems are still defined based on a population of solutions, however these solutions are constructed probabilistically, with each solution being generated from scratch. This has the two-fold effect of reducing the capacity of CMSA to learn from the best individual solution found so far, and also makes it impractical when trying to solve large-scale, or very complicated, problems for which constructing a feasible solution is extremely time consuming, such as the PCPSP that is considered here. Merge search avoids this issue by heuristically constructing an initial solution and then sampling its neighbourhood in order to generate a population, which is then used to define its subproblems. The effect of this is to make the time taken to generate the population of solutions dependent on the local search algorithm used to sample the neighbourhood of a given feasible solution which is often much faster than the algorithm used to find feasible solutions from scratch. Of course, this leaves Merge Search potentially susceptible to being overly sensitive to the quality of the initial solution --- as discussed previously in this section --- however, this can be mitigated by increasing the diversity of the population, or further splitting the merge partitions. 

The second area where there is a significant divergence between the two methods is in the aggregation of decision variables. CMSA uses its population of constructed solutions to determine the variables that are to be included in its reduced subproblem. If a variable is represented by an element of one of the candidate solutions in the population, it is automatically included in the reduced subproblem. These variables are added to the subproblem individually, and as such this aspect functions similarly to large neighbourhood search (LNS). One consequence of this is that the subproblems can become very large, especially for problems where there are naturally many non-zero values in a solution. To combat this, a so-called ``ageing'' mechanism is introduced in CMSA to ensure that elements that have not been useful in producing good quality solutions recently are removed from the pool of solution elements. In contrast, Merge Search uses information from across the entire population to determine which variables are added into the reduced subproblem as well as to aggregate variables that share common values across the population, and so likely share some kind of dependency. This grouping allows for more compact subproblems and a much larger region of the search space can be covered for the same computational power. The trade-off for this is much coarser-grained solutions, however this can be alleviated by the introduction of random splitting to these groups to help escape local optima.

% there are some significant differences. Most
% important is that the CMSA subproblems, while still defined based on a
% population of solutions, does not involve any aggregation of variables so that these
% subproblems can become very large unless the problem naturally has few
% non-zero values in the solution. Hence, the method as proposed in \cite{Blum2016}
% would be completely impractical for the problem considered here, because it has
% millions of non-zero variable values in solutions to large instances.
% Furthermore, CMSA relies on random construction to create populations of
% solutions, which reduces the ability to learn from the best solution found
% so far. The reliance purely on populations of feasible solutions (without the random splitting) also means that it relies purely on the random solution construction to escape any local optima. Finally, CMSA introduces an aging process to manage solution
% components, which is absent here.


\section{Applying Merge Search to solve problems}
Merge Search is presented here as a general hybrid meta-heuristic framework for solving constrained combinatorial optimisation problems. In order to demonstrate its effectiveness and versatility, Merge Search is applied to two problems from two different domains: the constrained pit-limit (CPIT) problem, an abstraction of a real-life problem from open-pit mining; and the Steiner tree problem in graphs (STPG), a famous NP-Complete problem which is very well-studied in the literature.

\subsection{Open-pit mining problems}
Open-pit mining is a very important industry in Australia and around the world~\cite{singh}. Two of the most critical tasks within the life-cycle of an open-pit mine is planning and production scheduling. These tasks allow the mine operator to estimate the total value of the mine over its life and also to identify areas for excavation that will yield the most value. Proper planning of a mine ensures maximum profit for the operator and, because this is typically talked about in the hundreds of millions of dollars, it is an excellent application for optimisation techniques as very small changes in efficiency can still translate to significant sums of money.\par

In order to model something so complex as a combinatorial optimisation problem, the earth to be mined (known as the \emph{orebody}) is typically discretised into a three-dimensional array of \emph{blocks} with each assigned a value based on the ore content and the cost required to excavate it. These values are calculated by taking core samples and using geological and statistical methods to estimate the value of each block. The aim is to maximise the net present value (NPV) of the mine by determining the set of blocks to extract and the order in which to extract them~\cite{Meagher2014}.\par

Problems in mine planning and production scheduling are very large and tend to have few side-constraints (often well under a hundred), but many blocks and many, many more precedence constraints governing when blocks can be mined. This means traditional mathematical solvers are unable to solve these problems without first using some form of decomposition, making these problems perfect candidates for hybrid meta-heuristics, despite there being very little in the literature.\par

Due to the sensitive nature of information surrounding mining enterprises, obtaining problem data for academic research can be challenging, however the website \emph{minelib}~\cite{espinoza_minelib:_2012} has a repository of problems and results that are freely available to the general public. These sets contain data for versions of the problem such as the ultimate pit limit (UPIT), constrained pit-limit and the precedence constrained production scheduling problem (PCPSP). It is the CPIT problem that will be the focus of these experiments.\par

\subsubsection{Solving CPIT with Merge Search}

\cite{Kenny:2017} describe a novel representation for the this type of problem and a greedy randomised adaptive search procedure (GRASP) algorithm for solving it. They build on this with local search operator for the CPIT problem, and use it in a simple merge search algorithm that operates without variable partitioning or random splitting~\cite{Kenny:2018}. By incorporating a variable partitioning mechanism, a much larger region of the search space is able to be explored for the same computational budget~\cite{Kenny:2019}. They exploit the structure of the problem, and treat many decision variables as a single group. This allows the size of the mixed-integer programming sub-problem to be greatly reduced and hence, the time required to solve it.\par

The algorithm presented in this paper is an exentsion of this work, with two additions: a random splitting heuristic, to allow greater granularity in the solutions produced; and a ``solution polishing'' technique, based on the local improvement heuristic from the GRASP algorithm.

% \cmnt[angus]{is this enough information? should i include the pseudocode or something?}

\subsection{The Steiner tree problem in graphs}

The Steiner tree problem in graphs (STPG) is a classic problem in the field
of combinatorial optimisation, the decision version being one of the original 
21 NP-complete problems outlined by \cite{np:karp} in his seminal paper.
Aside from being a fundamental problem in the abstract world of computing theory, 
the STPG has numerous real-world applications in communications, pipeline and transport network
design, computational biology and very
large-scale integrated circuit (VLSI) design~\cite{vlsi:cho}.\par 
Despite being around for centuries~\cite{history:brazil}, the STPG has also gained
much attention in recent decades due to it being the mathematical structure behind 
multicast networking problems~\cite{steiner:hwang}. Exact methods for solving the STPG 
have been developed using techniques such as integer linear programming (ILP), 
lagrangian relaxation and primal-dual strategies~\cite{pd:polzin}; however these approaches suffer from
exponential worst-case computation times which can make some large-scale instances intractable. The current state-of-the-art 
exact approaches to solving the STPG are hybrid~\cite{algo:polzin,algo:daneshmand}; 
several algorithmic, graph reduction, metaheuristic and 
mathematical programming techniques, working together to produce provably optimal solutions
in a much faster time than traditional optimisation techniques alone.
When good quality, but not necessarily optimal, solutions are required in a reasonable amount of
time, metaheuristics and decomposition techniques have been used to tackle this problem; some of the
more successful approaches in this manner have been memetic algorithms \cite{memetic:klau}, ant colony optimisation
\cite{aco:singh,acogroup:nguyen}, local search techniques 
\cite{fastls:uchoa,effectivels:wade} and voronoi-based 
decomposition heuristics \cite{partition:leitner}.\par

The STPG is extremely well-travelled, with many sophisticated pre-processing techniques and methods of finding good quality solutions already available in the literature. The purpose of including it in this paper is not to improve the current state-of-the-art results; but to provide a simple test-bed, upon which the properties of the proposed merge search algorithm can be investigated --- much more easily than with a more complex, real-world problem such as the constrained pit-limit problem, mentioned above.

\subsubsection{Solving the STPG with Merge Search}

First described by \cite{kp:dowsland}, but subsequently used widely by 
many researchers, is the so-called key path neighbourhood.
A key path is defined as a path within a Steiner tree where the two end vertices are
either terminal vertices or vertices of degree at least 3; all intermediate 
vertices (if any) are of degree 2 and are not terminal. The useful property of such
structures is that their removal from a solution to the STPG will always result in two disconnected
trees which can then be subsequently reconnected. This local search neighbourhood was extended by~\cite{Kenny:2016}, by adding a so-called ``jump'' operator which aids in escaping local minima, and is used as the basis of the Merge Search algorithm presented in this paper.

The problem instance is first pre-processed using some of the techniques described in~\cite{pp:duin,pp:uchoa,stpg-reduce} to reduce the number of decision variables. An initial solution is constructed and a neighbouring population produced, using the methods described in~\cite{Kenny:2016}. This population of solutions is used to partition the decision variables in the manner described in Section~\ref{sec:MS}.

\begin{figure}[h]
\centering
\resizebox{0.9\linewidth}{!}{{}
    \includegraphics{stpg_merge.tikz}
    }
    \caption[Partitioning variables by merging solutions]{By merging two solutions (top left and right), the decision variables are partitioned into four disjoint sets, indicated here (bottom) by: red; blue; red and blue; and no colour.}\label{fig:stpg-partition} 
\end{figure}

These partitions can be split by selecting an arbitrary partition, and an arbitrary variable in that partition, and separating all variables representing vertices and edges that are connected to that arbitrary variable. This ensures that any partition is split such that all variables removed from the original partition comprise a connected sub-graph, and as such should be able to be included in the resultant merged solution on its own, without the original partition. If a partition is split such that all variables are separated, leaving the original partition empty, then the last variable added to the split is kept in the original partition. As the connected variables are added using DFS, this is always a variable on the boundary, and so this method will not suffer from the problems present with truely random splitting. 

Having partitioned and split the decision variables, the resulting reduced subproblem can be solved using a version of the~\cite{goemans} MIP formulation that has been modified be expressed in terms of a single set of partition variables, instead of edge and vertex variables.

% \cmnt[angus]{should i actually include the formulation (or pseudocode) here?}

\section{Experiments}
This section details the experimental set up, the experiments performed and finally presents the results and discusses the implications those results suggest.
\subsection{Datasets}
\subsubsection*{The CPIT problem:}
Table~\ref{tab:datasets} details the properties of the problem instances used to test the algorithm in this paper. The instance name is given in the first column; followed by the number of blocks in the orebody model; the number of precedence arcs for each instance is provided in the third column; the fourth column gives the total number of time periods available; the number of decision variables is shown in the second last column; with the last column giving the total number of constraints in the problem model.\par
%
\begin{table}[h!]
\centering
\caption{Characteristics of \emph{minelib}~\cite{espinoza_minelib:_2012} datasets.}\label{tab:datasets}
\begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{lrrrrr} \toprule
Instance & Blocks & Precedences & Periods & Variables & Constraints\\
\hline
\texttt{newman1} & 1,060 & 3,922 & 6 & 6,360 & 29,904\\
\texttt{zuck\_small} & 9,400 & 145,640 & 20 & 188,000 & 3,100,840\\
\texttt{kd} & 14,153 & 219,778 & 12 & 169,836 & 2,807,196\\
\texttt{zuck\_medium} & 29,277 & 1,271,207 & 15 & 439,155 & 19,507,290\\
\texttt{marvin} & 53,271 & 650,631 & 20 & 1,065,420 & 14,078,080\\
\texttt{zuck\_large} & 96,821 & 1,053,105 & 30 & 2,904,630 & 34,497,840\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
%
A few instances from the full \emph{minelib} dataset were omitted due to missing files, referencing more than two resources or being too large for the algorithm in its current incarnation.

\subsubsection*{The Steiner tree problem in graphs:}

The experiments were were carried out on categories $B$, $C$, $D$ and $E$ of the STPG problems from the \textit{steinlib} library~\cite{steinlib}, a standard dataset used by many researchers. The smaller-scale, category $B$ instances are 18 randomised networks with 50 to 100 vertices and 63 to 200 edges. The $C$ and $D$ datasets consists of 20 larger randomised networks, with 500 and 1,000 vertices, respectively, and between 625 to 25,000 edges. Finally, the $E$ dataset contains the largest instances, each with 2,500 vertices and between 3,125 and 62,500 edges. Details of the individual instances can be found in Table~\ref{tab:stpgprobs1}. 

The optimal solutions for these instances are reported in the library and 
have been proven by exact methods such as branch-and-bound with graph reduction techniques.\par

\begin{table*}[h]
\centering
\caption[Characteristics of category $B$, $C$, $D$ and $E$ instances from the \textit{steinlib} database]{Characteristics of category $B$ and $C$ instances from the \textit{steinlib} database. $|V|$: number of vertices; $|E|$: number of edges; $|T|$: the number of terminals; and,
         opt: the total cost of the optimal solution.}\label{tab:stpgprobs1}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lrrrr|lrrrr|lrrrr|lrrrr} \toprule
\multicolumn{5}{l}{Category $B$} & \multicolumn{5}{l}{Category $C$}&\multicolumn{5}{l}{Category $D$} & \multicolumn{5}{l}{Category $E$}\\ 
\cmidrule(lr){1-5} \cmidrule(lr){6-10}\cmidrule(lr){11-15}\cmidrule(lr){16-20}
Inst.&$|V|$&$|E|$&$|T|$&opt& Inst.&$|V|$&$|E|$&$|T|$&opt&Inst.&$|V|$&$|E|$&$|T|$&opt& Inst.&$|V|$&$|E|$&$|T|$&opt\\ \hline
$b01$ & 50 & 63 & 9 & 82 & $c01$ & 500 & 625 & 5 & 85&  $d01$ & 1,000 & 1,250 & 5 & 106 & $e01$ & 2,500 & 3,125 & 5 & 111\\
$b02$ & 50 & 63 & 13 & 83& $c02$ & 500 & 625 & 10 & 144&  $d02$ & 1,000 & 1,250 & 10 & 220 & $e02$ & 2,500 & 3,125 & 10 & 214\\
$b03$ & 50 & 63 & 25 & 138& $c03$ & 500 & 625 & 83 & 754&  $d03$ & 1,000 & 1,250 & 167 & 1,565& $e03$ & 2,500 & 3,125 & 417 & 4,013\\
$b04$ & 50 & 100 & 9 & 59& $c04$ & 500 & 625 & 125 & 1,079&  $d04$ & 1,000 & 1,250 & 250 & 1,935& $e04$ & 2,500 & 3,125 & 625 & 5,101\\
$b05$ & 50 & 100 & 13 & 61& $c05$ & 500 & 625 & 250 & 1,579&  $d05$ & 1,000 & 1,250 & 500 & 3,250& $e05$ & 2,500 & 3,125 & 1,250 & 8,128\\
$b06$ & 50 & 100 & 25 & 122& $c06$ & 500 & 1,000 & 5 & 55&  $d06$ & 1,000 & 2,000 & 5 & 67& $e06$ & 2,500 & 5,000 & 5 & 73\\
$b07$ & 75 & 94 & 13 & 111& $c07$ & 500 & 1,000 & 10 & 102&  $d07$ & 1,000 & 2,000 & 10 & 103& $e07$ & 2,500 & 5,000 & 10 & 145\\
$b08$ & 75 & 94 & 19 & 104& $c08$ & 500 & 1,000 & 83 & 509&  $d08$ & 1,000 & 2,000 & 167 & 1,072& $e08$ & 2,500 & 5,000 & 417 & 2,640\\
$b09$ & 75 & 94 & 38 & 220& $c09$ & 500 & 1,000 & 125 & 707&  $d09$ & 1,000 & 2,000 & 250 & 1,448& $e09$ & 2,500 & 5,000 & 625 & 3,604\\
$b10$ & 75 & 150 & 13 & 86& $c10$ & 500 & 1,000 & 250 &1,093&  $d10$ & 1,000 & 2,000 & 500 & 2,110& $e10$ & 2,500 & 5,000 & 1,250 &5,600\\
$b11$ & 75 & 150 & 19 & 88& $c11$ & 500 & 2,500 & 5 &32&  $d11$ & 1,000 & 5,000 & 5 & 29& $e11$ & 2,500 & 12,500 & 5 &34\\
$b12$ & 75 & 150 & 38 & 174& $c12$ & 500 & 2,500 & 10 & 46&  $d12$ & 1,000 & 5,000 & 10 & 42& $e12$ & 2,500 & 12,500 & 10 & 67\\
$b13$ & 100 & 125 & 17 & 165& $c13$ & 500 & 2,500 & 83 & 258&  $d13$ & 1,000 & 5,000 & 167 & 500& $e13$ & 2,500 & 12,500 & 417 & 1,280\\
$b14$ & 100 & 125 & 25 & 235& $c14$ & 500 & 2,500 & 125 & 323&  $d14$ & 1,000 & 5,000 & 250 & 667& $e14$ & 2,500 & 12,500 & 625 & 1,732\\
$b15$ & 100 & 125 & 50 & 318& $c15$ & 500 & 2,500 & 250 & 556&  $d15$ & 1,000 & 5,000 & 500 & 1,116& $e15$ & 2,500 & 12,500 & 1,250 & 2,784\\
$b16$ & 100 & 200 & 17 & 127& $c16$ & 500 & 12,500 & 5 & 11&  $d16$ & 1,000 & 25,000 & 5 & 13& $e16$ & 2,500 & 62,500 & 5 & 15\\
$b17$ & 100 & 200 & 25 & 131& $c17$ & 500 & 12,500 & 10 & 18&  $d17$ & 1,000 & 25,000 & 10 & 23& $e17$ & 2,500 & 62,500 & 10 & 25\\
$b18$ & 100 & 200 & 50 & 218& $c18$ & 500 & 12,500 & 83 & 113&  $d18$ & 1,000 & 25,000 & 167 & 223& $e18$ & 2,500 & 62,500 & 417 & 564\\
&&&&& $c19$ & 500 & 12,500 & 125 & 146 &$d19$ & 1,000 & 25,000 & 250 & 310& $e19$ & 2,500 & 62,500 & 625 & 758\\
&&&&& $c20$ & 500 & 12,500 & 250 & 267 &$d20$ & 1,000 & 25,000 & 500 & 537& $e20$ & 2,500 & 62,500 & 1,250 & 1,342\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Pre-processing}
\subsubsection*{The CPIT problem}
The ``time-expanded'' method of representing the CPIT problem given in~\cite{Kenny:2017} has one significant drawback: it exponentially increases the size of the problem instances. In this technique, each block-time pair is associated with a single decision variable and a solution is represented as a closure which partitions this expanded graph. In order to mitigate this effect, the structure of the problem can be exploited to apply pre-processing to reduce the number of variables and constraints in each problem instance.

First, a relaxed version of the problem, called UPIT, is solved to determine the set of blocks that are worth extracting at all; then the earliest and latest possible times for extraction for each block are computed. Because a condition of extracting a block is that all blocks above it must be extracted first, blocks that are deep cannot be extracted too early in the process --- because there is not enough time to reach them --- and similarly, blocks that are close to the surface cannot be extracted too late --- as there will not be enough time to reach the blocks below. More detail about these pre-procesing methods can be found in~\cite{Kenny:2017}.

\subsubsection*{The Steiner tree problem in graphs}
There are many techniques available in the literature for pre-processing instances of the STPG, ranging from the very simple to the very complicated. As the experiments on the STPG for this study were not intended to improve the current state-of-the-art, but merely to investigate the properties of merge search and prove its versatility; the purpose of pre-processing the problem instances was simply to make them more manageable for the algorithms being tested. To this end, three basic ones were chosen from the literature: removing non-terminal vertices of degree 1; removing edges where a shorter path exists in the graph; and, replacing the edges incident to non-terminal vertices of degree two with a single edge. For further reading on graph reduction techniques for pre-processing instances of the STPG, see \cite{pp:duin,pp:uchoa,stpg-reduce}.

\subsection{Experimental setup}
The experiments were carried out on an \emph{Intel{\textsuperscript{\textregistered}} Core{\textsuperscript{TM}} i5-2320} processor (3.0GHz) with 24GB RAM running Linux. All code was implemented in C++ with GCC-4.8.0. CPLEX Studio 12.7, operating with a single thread due to the need for callbacks, was employed as the mixed integer programming (MIP) solver. For the CPIT problem experiments, the boost library implementation of the Boykov-Kolmogorov algorithm was used to solve the UPIT sub-problem during the pre-processing stage.

\subsubsection*{The CPIT problem:}
The merge search algorithm was compared against the baselines of the results published on the \emph{minelib}~\cite{espinoza_minelib:_2012} website and the results from using a greedy randomised adaptive search procedure (GRASP) heuristic for the precedence constrained production scheduling problem (PCPSP)~\cite{Kenny:2017}, adapted for use with the CPIT problem. Also provided are the results for a variant of the merge search algorithm that uses variable grouping but no random splitting, previously published in~\cite{Kenny:2018} --- these results are included in order to illustrate the effect that random splitting has on solution quality. 

Each algorithm was run 20 times on each instance, recording the mean objective value and standard deviation of the resulting solution produced by each run. Finally, the last experiment performed was to use the local improvement heuristic from the GRASP algorithm to ``polish'' the best result produced by the merge algorithm.

\subsubsection*{The Steiner tree problem in graphs:}
The merge search algorithm was compared against three separate baseline algorithms, the greedy randomised adaptive search (GRASP) heuristic, pure local search and pure MIP. The method of constructing initial solutions was different between the GRASP and merge search algorithms, so pure local search and pure MIP were included to ensure any improvement was not solely based on this factor. Aside from population merging, the merge search algorithm comprises two main components, local search and MIP search; so by isolating these two factors, it can be shown that merge search is greater than the sum of its parts.

Each algorithm was run 30 times on each (pre-processed) instance from the datasets, recording the mean objective value and standard deviation across all of the runs. All instances in the datasets used are supplied with their optimal objective values and this is used for the main stopping criteria. Otherwise, the stopping criteria of the merge search algorithm is generally dictated by the number of seconds spent in the MIP search so the time taken for each search is not provided, as it does not give much information about the performance of the algorithm. However, special mention is made when the algorithm terminated early for all runs.

Additional experiments were performed to investigate the difference between using the random and deterministic solution construction heuristics and the effect of population size on the size of the reduced sub-problem. In order to preserve space, a representative subset of the problem instances is used to illustrate the outcome of these experiments; however, these results are not ``cherry-picked'' and the full tables are available in the appendix. 
% \cmnt[angus]{(will add appendix later)}
% \cmnt[angus]{should i add some details about the parameters used here?}

\section{Results and discussion}
\subsection{The CPIT problem:}
Table~\ref{tab:mine:main} provides the results of the three algorithms tested on the six problem instances from the \emph{minelib} dataset, along with the linear programming (LP) upper bound and the current best solution as published on the \emph{minelib} website, ordered from smallest problem instance to largest.

In this table, it can be seen that the two merge search algorithms consistently produce better results than the \emph{minelib} and the GRASP heuristic results, except for \newman{} and \zucklarge{}. The results for \newman{} are similar because the problem instance is so small and it can be assumed that the objective value of 2.418E+07 is quite close to the optimal solution, as all three algorithms agree on this as an average value and when the search is performed without any stopping criteria, the maximum objective value that is found is 2.41798E+07.

\begin{table*}[h!]
\centering
\caption[Results on \emph{minelib} dataset instances]{Results on \emph{minelib} dataset instances. Given are the LP upper bound, current best solution from the \emph{minelib} website, the mean objective values (\(\mu\)) and standard deviations (\(\sigma\)) for the GRASP heuristic, merge search without random splitting and full merge search heuristic.% The final column gives the result of using the local improvement heuristic from the GRASP algorithm to polish the best solution found with merge search.
}\label{tab:mine:main}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lrrrrrrrrrr} \toprule
 & \multicolumn{1}{c}{LP UB} & \multicolumn{1}{c}{\emph{minelib}} & \multicolumn{2}{c}{GRASP heuristic} & \multicolumn{2}{c}{merge search (no splitting)} & \multicolumn{2}{c}{merge search (with splitting)}\\
\cmidrule(lr){2-2}\cmidrule(lr){3-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
Instance & & & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}  \\ \midrule
%
\input mine/tables/main_table.tex
%
\bottomrule

% \multicolumn{5}{l}{$^*$ this result was beaten by polishing the best merge search solution with the GRASP local improvement heuristic (Table~\ref{tab:mine:polish}).}
\end{tabular}
\end{adjustbox}
\end{table*}
Comparing the results of the two variants of merge search illustrates the effect that random splitting has on the quality of the solution produced. For smaller problem instances, the two algorithms produce very similar quality solutions, indicating that the random splitting has little effect. 
\footnotetext{{This result was beaten by polishing the best merge search solution with the GRASP local improvement heuristic (Table~\ref{tab:mine:polish}).}}% footnote mark is in table source
However, as the size of the problem increases, the effect of the random splitting becomes more pronounced, with the biggest effect being on the two largest instances \zuckmed{} and \zucklarge{}\footnote{Although it is technically a larger problem than \zuckmed, solving the UPIT problem on the \marvin{} instance, as part of the pre-processing stage, eliminates many blocks and makes its effective size much smaller than \zuckmed.}. 

If there are no overlaps at all between variables across solutions, then the partitions in the reduced sub-problem are simply the set differences of the variables in each solution and the seed solution. In this extreme case, the merge operation is unable to produce a solution that does not already exist in the population, unless some random splitting of the partitions is performed. As the size of the problem instance decreases, the likelihood that there will be overlaps between variables across solutions increases. These overlaps in the variable sets produce splits in the partitions when producing the reduced sub-problem, reducing the need for additional random splitting to produce a different solution to those already existing in the generated population. This is not to suggest that random splitting would not be beneficial; but the problem with random splitting is that it is \emph{random}, and there is no way of guaranteeing that a particular split will improve the quality of a solution after merging, any more than a split produced by overlapping variable sets will.

The GRASP heuristic produces consistently worse results than merge search. This is expected as the sliding window heuristic used in the local improvement phase of the algorithm is better suited to incrementally improving a good solution than turning a mediocre solution into a good one, as it only operates on a small subset of the variables at one time. This means that it is good for ``tweaking'' a solution by shifting the time that a block is mined forward or backward one or two periods; but it is no good if the time that the block is mined must be moved by many periods, as this would take a lot of passes to achieve.

\subsubsection*{Convergence behaviour}

This idea of GRASP being better suited to incrementally improving an already good solution is demonstrated in Figure~\ref{plot:mine:runtime}. This figure gives the plots of the convergence behaviour of both the GRASP algorithm and merge search on all six of the CPIT instances. It can be seen in these plots that merge search converges quicker in nearly all instances, except for \dmine{}. 

\begin{figure*}[!]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/newman1_conv.tikz}
    }
    \vspace*{-5mm}\caption{\newman{}}
    \label{plot:newman-conv}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/zuck_small_conv.tikz}
    }
    \vspace*{-5mm}\caption{\zucksmall{}}
    \label{plot:zuck-small-conv}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/kd_conv.tikz}
    }
    \vspace*{-5mm}\caption{\dmine{}}
    \label{plot:kd-conv}
    \end{subfigure}
    %
    \\[5mm]
    %
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/zuck_med_conv.tikz}
    }
    \vspace*{-5mm}\caption{\zuckmed{}}
    \label{plot:zuck-med-conv}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/marvin_conv.tikz}
    }
    \vspace*{-5mm}\caption{\marvin{}}
    \label{plot:marvin-conv}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/conv/zuck_large_conv.tikz}
    }
    \vspace*{-5mm}\caption{\zucklarge{}}
    \label{plot:zuck-large-conv}
    \end{subfigure}
    \caption[Convergence plots for merge search on CPIT instances]{Convergence plots for merge search on CPIT instances. Merge search data is shown in blue and GRASP is shown in red.}
    \label{plot:mine:runtime}
\end{figure*}

The difference here, is that the initial solution is already quite close in value to the resultant solution, so there are not a lot of improvements that can be made. This means that the more exhaustive search for small improvements of the GRASP algorithm is more likely to be effective, early on, than the more global search of the merge search algorithm, which relies on the stochastic natures of the local search operator and arbitrary splitting heuristic to find its improvements. Merge search does get there in the end --- and manages to find a slightly better solution in this case --- but it takes longer and with a more gentle curve. For all the other instances, where the gap between the initial solution and the resultant one is much larger, merge search is demonstrably more suited to the task.

It can also be seen from these plots that once the algorithm has seemingly converged, it can sometimes find a way out of the local optima and find a much better solution. This is evidenced in the plots for \zuckmed{}~(Figure ~\ref{plot:zuck-med-conv}), \marvin{}~(Figure~\ref{plot:marvin-conv}) and also Figure~\ref{plot:polish.med1} below. This behaviour is expected in such large problem instances, as once it has started to converge there are many ways to make the solution worse, but only a few to make it better; but once it has found a way out of the local optima, often several other improving moves will become apparent as well.

\subsubsection*{Runtime information}

Table~\ref{tab:mine:runtime} gives the runtime information (wall time and CPU time) for both the GRASP and merge search algorithms. As they use a MIP solver to solve their restricted sub-problems, the runtime of both GRASP and merge search is reasonably easily configured by controlling how long the solver is allowed to run for each iteration. The parameters of the GRASP algorithm were chosen so that the search would take roughly the same amount of time as that of the merge search --- and for the most part, they are pretty similar. 

\begin{table}[h!]
\centering
\caption[Runtime information for experiments on CPIT]{Runtime information for experiments on CPIT. Given are the mean (\(\mu\)) and standard deviations (\(\sigma\)) of the wall and CPU time for the GRASP and merge search algorithms, in seconds.}\label{tab:mine:runtime}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{lrrrrrrrr} \toprule
 & \multicolumn{4}{c}{GRASP} & \multicolumn{4}{c}{merge search}\\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & \multicolumn{2}{c}{Wall time [s]} & \multicolumn{2}{c}{CPU time [s]} & \multicolumn{2}{c}{Wall time [s]} & \multicolumn{2}{c}{CPU time [s]}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}
Instance & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}& \multicolumn{1}{c}{\(\sigma\)}& \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}& \multicolumn{1}{c}{\(\sigma\)}\\ \midrule

\input mine/tables/runtime_table.tex

\bottomrule
\multicolumn{9}{l}{\(^*\)population size for \zucklarge{} set at 500 for merge search due to insufficient memory.}
\end{tabular}
\end{adjustbox}
\end{table}

The two instances that are significantly different in runtime between GRASP and merge search are \dmine{} and \zucklarge{}. The reason that \zucklarge{} is so different is that it was impossible to run the merge search algorithm with a population of 1,000 on such a large problem instance, due to memory issues (even with 23 GB of RAM!); so a smaller population size of 500 was used, which took much less time to produce and to compute the merge partitions.

The reason for the differences in runtime on \dmine{} is illustrated in Figure~\ref{plot:mine:time}.
%
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/time/zuck_small_grasp_time.tikz}
    }
    \caption{}
    \label{plot:time.1}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/time/zuck_small_merge_time.tikz}
    }
    \caption{}
    \label{plot:time.2}
    \end{subfigure}
    \caption[Plot of wall time {[s]} for \zucksmall{} and \dmine{} instances]{Plot of wall time [s] for \zucksmall{} (red) and \dmine{} (blue) instances. Sub-figure (\subref{plot:time.1})~shows GRASP algorithm data and (\subref{plot:time.2})~shows merge search data.}
    \label{plot:mine:time}
\end{figure}
%
This figure shows plots of the amount of wall time in seconds consumed per iteration of the respective algorithms. The GRASP algorithm is measured by ``window movements'', which counts the number of times the window has incremented forwards or backwards along the solution. The merge search algorithm is measured by iterations; each the time for each iteration is measured at the point when a new candidate solution is generated for the population, so it is clear to see that, with a population size of one thousand, every thousandth iteration will include an extra amount of time to include the merge operation itself. 

It is important to remember here that the \dmine{} instance only has a maximum of 12 periods allowed, while \zucksmall{} has 20. So the difference in time between GRASP and merge search is easy to explain, because GRASP is dependent on the number of periods in the solution as that determines how many window movements will be made; whereas merge search will generate the same size population and perform the same number of merges, no matter how many periods are allowed. This is demonstrated very clearly in Figure~\ref{plot:time.2}, where there is very little difference between the two instances. It can be seen that the population is generated reasonably quickly (indicated by the flat parts of the curve) and then each point where there is a sudden jump in the amount of time taken indicates that a merge operation has taken place.

But this is not the end of the story. In these two examples, GRASP took 2,979 seconds to complete the search on \dmine{} and it took 4,180 to complete the search on \zucksmall{}. If there was a direct linear relationship between time taken to complete the search and number of periods, the amount of time GRASP should take to complete its search on \zucksmall{} should be \(\frac{2,979\times20}{12} = 4,965\). So where did the other 13 minutes go?

Looking at Figure~\ref{plot:time.1}, the main thing that can be noticed here is that the shape of the plot for \zucksmall{} is significantly different to the shape of the plot for \dmine{}. The regular flat spots in the plot for \zucksmall{} indicate that there is a group of periods for that problem that are not used by the solution, or do not contain any blocks that can be moved around, and therefore the MIP solver will not take the full amount of allowed time to solve the reduced sub-problem. It so happens that the solution to \zucksmall{} does not use any period after period 16, so the first and last few window moves of each full pass take very little time at all. In contrast the \dmine{} instance uses periods right up until period 10, so there are fewer window moves that will be skipped over, as evidenced by the straighter line on the plot.

\subsubsection*{Solution polishing}

As the quality of the solutions produced by the random construction heuristic is not extremely high, too much of the local improvement phase is spent moving the times that blocks are mined over long temporal distances; so the algorithm is unable to produce high quality solutions in the allowed computational budget. It was observed however, that the quality of the solutions produced by the GRASP algorithm greatly depended on the quality of the initial solution that was constructed. So it was decided to see what would happen if the local improvement heuristic from the GRASP algorithm was applied to ``polish'' the best solution obtained by the merge search algorithm. These results are given in Table~\ref{tab:mine:polish}.

\begin{table}[h!]
\centering
\caption[Results of polishing the best merge search solution]{Results of polishing the best merge search solution with the local improvement heursistic from the GRASP algorithm. Given is the LP upper bound and current best solution from the \emph{minelib} website, the objective value of the best solution produced by the merge search algorithm and the value of that solution when polished by the local improvement heuristic.}\label{tab:mine:polish}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{lrrrr} \toprule
Instance & \multicolumn{1}{c}{LP UB} & \multicolumn{1}{c}{\emph{minelib}} & \multicolumn{1}{c}{merge search} & polished merge\\ 
\midrule
%
\input mine/tables/polish_table.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

This table clearly demonstrates that the sliding window heuristic is very effective in improving solutions produced by merge search, even surpassing the \emph{minelib} result for \zucklarge{} --- something that merge search could not achieve on its own, without polishing. For these experiments, the best solution from the merge search runs is taken and then three passes of the sliding window heuristic is applied. Although doing this effectively increases the allowed computational budget, the quality of the solutions produced by doing this is better than if the computational budget is increased for either of the two algorithms on their own. In the case of the GRASP algorithm, the quality of the solution at the end of its run is still too low to be able to improve the objective value by very much in only three passes. In the case of the merge search algorithm, because both the population generation and merge partition splitting occurs stochastically, as the quality of the solution increases the chances of finding an improving solution, or partition split, by chance decreases. This means that the search is likely to have converged by the end of its run, so increasing the computational budget at this point is unlikely to produce much of an improvement in the quality of the solution, if any at all.

Applying the sliding window heuristic at the end of the merge search process solves both of these problems. The sliding window heuristic performs a much more methodical and exhaustive search to find small improvements that can be made to the solution which might be difficult to find by random sampling, in such a large search space. Additionally, by starting the sliding window heuristic with a much higher quality initial solution, computational effort does not need to be wasted on the ``low hanging fruit'' that can be found easily by sampling the search space. As the heuristic only considers a small subset of the variables at a time, it cannot make large changes to the solution in a single iteration. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/polish/zuck_med_polish.tikz}
    }
    \vspace*{-5mm}\caption{Merge with polishing on \zuckmed{}}
    \label{plot:polish.med1}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/polish/zuck_med_grasp_polish.tikz}
    }
    \caption{Detail of polishing phase}
    \label{plot:polish.med2}
    \end{subfigure}
    %
    \quad
    %
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/polish/zuck_large_polish.tikz}
    }
    \vspace*{-5mm}\caption{Merge with polishing on \zucklarge{}}
    \label{plot:polish.large1}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{mine/plots/polish/zuck_large_grasp_polish.tikz}
    }
    \vspace*{-5mm}\caption{Detail of polishing phase}
    \label{plot:polish.large2}
    \end{subfigure}
    \caption[Plot of merge search with solution polishing on \zuckmed{} and \zucklarge{} instances]{Plot of merge search with solution polishing on \zuckmed{} (top) and \zucklarge{} (bottom) instances. The left-hand figures show the entire merge search (blue) and polishing (red) phases; the right-hand figures show details of the polishing phases.}
    \label{plot:mine:polish}
\end{figure*}

Figure~\ref{plot:mine:polish} contains plots that illustrate this concept on two instances of the CPIT problem, \zuckmed{} and \zucklarge{}. The left-hand sub-figures show the full process with both merge phase (shown in blue) and polishing phase (shown in red). Figure~\ref{plot:polish.med1} demonstrates the power of intensification of the search with the sliding window heuristic. Here, the blue merge plot can seen to have effectively converged as there has been no significant improvement in objective value for over half an hour of wall time. Applying the sliding window heuristic to this ``converged'' solution results in an almost immediate improvement, followed by subsequent improvements, before this too converges to its local optimum.

Not as emphatic in demonstrating this point are the results presented in Figure~\ref{plot:polish.large1}. It can be seen in this figure that merge search was still a ways off from converging, although it had started to slow its progress. It could be argued that, were merge search allowed to continue, it would have found the same solution eventually. Although, it can be seen in the plot that applying the polishing heuristic to this unconverged solution did still result in an initial rapid increase in solution quality; and as merge search was beginning to tail off, it probably would have taken longer to reach its goal.

The results from these experiments suggest that, while merge search is adept at finding a good quality global solution, it benefits from the addition of a more exhaustive local method to intensify its search. 

\subsubsection*{Updated state-of-the-art results}

When this research was first undertaken, the current published state-of-the-art results were indeed those available on the \emph{minelib} website. However, since then, there have been several developments as published in a recent paper by~\cite{minelib-improved} that has collated all reported improvements to the \emph{minelib} dataset.

According to this aggregation of results, recently there are three separate studies that have improved the most on the \emph{minelib} results for the CPIT problem instances used for this research~\cite{newman-improved,zuck-small-improved,JELVEZ20161169}. These improved results are given in Table~\ref{tab:mine:update} in terms of their LP gap, and are compared with the original \emph{minelib} results and the polished merge results from earlier in this section.

\begin{table}[h!]
\centering
\caption[Recent updates to state-of-the-art results for \emph{minelib} CPIT instances]{Recent updates to state-of-the-art results for \emph{minelib} CPIT instances. Given is the percentage gap between the LP upper bound and the best result achieved for each respective study, compared with the original \emph{minelib} results and the best results from this research.}\label{tab:mine:update}
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{lrrrrr} \toprule
& & & & \multicolumn{2}{c}{new results}\\ 
\cmidrule(lr){5-6}
Instance & \multicolumn{1}{c}{LP UB} & \multicolumn{1}{c}{\emph{minelib}} & \multicolumn{1}{c}{polished merge} & \multicolumn{1}{c}{publication} & \multicolumn{1}{c}{gap}\\ 
\midrule
%
\input mine/tables/updated_table.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

Although the results from this study do not improve on every known bound in the literature, they certainly are competitive with the state-of-the-art and actually do improve the current best-known bound for the \dmine{} and the \zucklarge{} instances.

\subsection{The Steiner tree problem in graphs:}

\begin{table*}[h]
% \begin{sidewaystable}
\centering
\caption[Results on \emph{steinlib} dataset \(B\) instances]{Results on \emph{steinlib} dataset \(B\), \(C\), \(D\) and \(E\) instances. Given are the known optimal solutions and mean objective values and standard deviations for pure local search, pure MIP, GRASP heuristic and merge search heuristic. Algorithms which managed to find the optimal solution in at least one of its runs are indicated by \opt{} next to the objective value.}\label{tab:stpg:results}
% \begin{adjustbox}{width=\textwidth}
\begin{tabular}{cc}
% \begin{table}[h!]
\centering
% \caption{Results on \emph{steinlib} dataset \(C\) instances, investigating the difference between the deterministic and random construction heuristics. Given are the average objective value and standard deviation for solutions produced by both heuristics over 20 runs and the difference between the averages, expressed as a percentage.}\label{tab:stpg:init:b}
% \begin{adjustbox}{width=\textwidth}
\begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{lrrrrrrrrr} \toprule
 &  & \multicolumn{2}{c}{pure LS} & \multicolumn{2}{c}{pure MIP} & \multicolumn{2}{c}{GRASP} & \multicolumn{2}{c}{merge search}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} 
Inst. & Opt. & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}& \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}\\ \midrule
%
\input stpg/tables/bd_data.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}
% \end{adjustbox}
% \end{table}
&
% \begin{table}[h!]
\centering
\begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{lrrrrrrrrr} \toprule
 &  & \multicolumn{2}{c}{pure LS} & \multicolumn{2}{c}{pure MIP} & \multicolumn{2}{c}{GRASP} & \multicolumn{2}{c}{merge search}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} 
Inst. & Opt. & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}& \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}\\ \midrule
%
\input stpg/tables/ce_data.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}

% \end{adjustbox}
% \end{table}
\end{tabular}
% \end{sidewaystable}
\end{table*}

Table~\ref{tab:stpg:results} provides the mean objective value and standard deviation for the experiments performed to compare the merge search algorithm to the three base-line algorithms of GRASP, pure local search and pure MIP. The best results for each problem instance are indicated in bold; and, as the optimal solution is known for each instance, if the algorithm managed to find the optimal solution in any of its runs, this is indicated by an asterisk --- clearly, if a result has an asterisk and a standard deviation of 0.00, this indicates the optimal solution was found in every run.

The results for the \(B\) dataset show that merge search demonstrably outperformed all of the algorithms it was tested against, as do the rest of the datasets. In this first dataset comprised of all the smallest sized instances (around 60 to 290 decision variables, pre-processed), merge search performed the best; managing to find the optimal solution in every run for every problem instance, with many terminating in the first couple of iterations. The next most successful algorithm was the pure MIP algorithm\footnote{It should be pointed out here that the pure MIP algorithm failed to produce even a single integer solution for most of the problem instances in the time allotted to it, if it was not given a heuristically constructed solution as a warm start. Therefore the success of the pure MIP algorithm over the GRASP algorithm for this dataset should be attributed to the fact that the MIP search was initially seeded with a higher quality solution than the GRASP algorithm.}, which just edges out pure local search and finally followed by GRASP at the end. The fact that both pure local search and pure MIP outperformed GRASP suggests that the quality of the initial solution is very important to the quality of the resultant solution --- especially in these smaller problem instances --- so, as pure local search and pure MIP both use the same, deterministic, solution construction as merge search, they are likely to perform better than GRASP, which uses the random solution construction heuristic.


The picture begins to change slightly with the results for the \(C\) dataset. Merge search is still the clear winner of the four here; although it has not managed to achieve the optimal solution in every single one of its runs and has actually failed to reach the optimal solution in a few of the instances. The big change here is the fact that pure MIP has fallen to last place for almost every single problem instance, even with the warm start seeding; and by the \(D\) dataset it is last in every problem instance. This suggests that, by the time the problem instances reach even this moderate size (between 400 and 5,280 decision variables), the problem is too large to be solved by a MIP solver alone, using comparable computing resources. This is evidenced by the fact that the standard deviation is 0.00 for many of the instances, suggesting that it had a hard time finding a better solution than the one it was seeded with. 

It is worth mentioning that some of the state-of-the-art results in the \emph{steinlib} library have been produced by a combination of pre-processing and integral LP formulation, so this does not mean that these results suggest the problems are too large for MIP solvers in general, merely that they are too large for the simple formulation used for this research. Better, more complicated MIP formulations for the STPG are available~\cite{goemans,stpg-improved,hypergraph}; however, these experiments are not intended to advance the state-of-the-art for STPG solvers, but to demonstrate the application of merge search to the STPG and that there is a non-trivial benefit that can be gained from doing so. If both pure MIP and merge search used better formulations, and more sophisticated pre-processing techniques, they would indeed be able to achieve better quality solutions; but one could expect a similar gulf in quality between the pure MIP and the hybrid meta-heuristic to emerge --- albeit, on much larger problem instances.

The trend of pure local search outperforming the GRASP heuristic continues throughout the rest of the datasets, with the exception of problem instances \texttt{c11}, \texttt{d06}, \texttt{d16} and \texttt{e02}. Differences between the mean objective values for these instances are small enough to suggest that these are anomalous and probably the result of the random construction heuristic finding a better initial solution to the deterministic heuristic, or simple luck during the local search. 

The fact that pure local search outperformed GRASP in nearly every problem instance suggests that the choice of initial solution is very important to the quality of the solution produced, as otherwise, these two algorithms are nearly identical; so the use of the deterministic construction heuristic is preferable to the random one. Added to this, the fact that merge search equals --- or outperforms --- pure local search in every problem instance suggests that there is indeed a benefit to creating a hybrid meta-heuristic using a MIP solver that solves a reduced sub-problem induced by merging a population of solutions produced by the local search neighbourhood. Finally, the fact that the merge search algorithm equals --- or outperforms --- the pure MIP algorithm in every problem instance suggests that these benefits that are gained are not simply from the addition of a MIP solver itself; and therefore, must be the result of the hybridisation and population merging processes.

\subsubsection*{Additional experiments}
\begin{table}[h!]
\centering
\caption[Deterministic vs. random construction heuristic results]{Representative sample of results across \emph{steinlib} dataset instances, investigating the difference between the deterministic and random construction heuristics. Given are the average objective value and standard deviation for solutions produced by both heuristics over 30 runs and the difference between the averages, expressed as a percentage of the larger value.}\label{tab:stpg:init:trunc} 
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{lrrrrr} \toprule
 & \multicolumn{2}{c}{deterministic} & \multicolumn{2}{c}{random}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
Instance & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & difference\\ \midrule
%
\input stpg/tables/trunc_init.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
Table~\ref{tab:stpg:init:trunc} illustrates the difference between using the deterministic solution construction heuristic and the random solution construction heuristic. It is very clear to see that the quality of the deterministic heuristic is much better than the average random construction heuristic, especially for the larger datasets, where the difference can be up to over \(95\%\). The full set of results are available in Table~\ref{tab:app:init} of the appendix.
% \cmnt[angus]{(will add appendix later)}

\begin{table}[t!]
\centering
\caption[Size of reduced sub-problem results]{Representative sample of results across \emph{steinlib} dataset instances, investigating the effect that population size has on the number of partitions in the reduced sub-problem. Given are the size of the instance (\(|V|+|E|\)) and average number of partitions and standard deviation over 30 runs for population sizes 50, 100, 1,500 and 10,000.}\label{tab:stpg:pop:trunc} 
\begin{adjustbox}{width=0.45\textwidth}
\begin{tabular}{lrrrrrrrrr} \toprule
 &  & \multicolumn{2}{c}{50} & \multicolumn{2}{c}{100} & \multicolumn{2}{c}{1,500} & \multicolumn{2}{c}{10,000}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} 
Instance & size & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}& \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(\mu\)}&\multicolumn{1}{c}{\(\sigma\)}\\ \midrule
%
\input stpg/tables/trunc_pop.tex
%
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

The results in Table~\ref{tab:stpg:pop:trunc} show the effect that changing the population size has on the number of partitions in the resultant reduced sub-problem, before random splitting is applied. It can be seen from the data that as the size of the population increases, the number of partitions also increases. This is to be expected, as two variables are included in the same partition only if they agree on values across the entire population of solutions; therefore, if more solutions are considered, the less likely variables are to agree across them all and the more partitions in the reduced sub-problem. Again, these are just a representative set of the problem instances, the full results are given in Table~\ref{tab:app:pop} of the appendix.

Figure~\ref{fig:stpg_pop_plots} shows some of the plots that were used in the sensitivity analysis done to determine an appropriate population size for these experiments. The number of partitions in the reduced sub-problem was plotted against population size from 0 to 5,000 and a population size of 1,500 was decided upon for two main reasons. The first reason is that it can be seen in the plots that although the number of partitions is proportional to the population size, it is not directly proportional, and after a certain inflection point there are significant diminishing returns in the number of partitions for population size. It can be seen from the plots that the position of this inflection point moves further to the right with the size of the problem and it was decided that 1,500 was a decent value that could be used across the whole dataset. It is slightly after the inflection point for most of the \(C\) dataset instances and slightly before the inflection point for most of the \(D\) dataset instances. The \(B\) dataset instances are small enough that it doesnt matter that 1,500 is probably too many and, although it falls very short of the \(E\) dataset instances inflection point, larger population sizes produce too many partitions for the MIP solver to handle.

\begin{center}
\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{stpg/plots/b10_pop_plot.tikz}
    }
    \caption{Instance \texttt{b10}}
    \label{fig:stpg_pop.1}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{stpg/plots/c10_pop_plot.tikz}
    }
    \caption{Instance \texttt{c10}}
    \label{fig:stpg_pop.2}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{stpg/plots/d10_pop_plot.tikz}
    }
    \caption{Instance \texttt{d10}}
    \label{fig:stpg_pop.3}
    \end{subfigure}
    \quad 
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{stpg/plots/e10_pop_plot.tikz}
    }
    \caption{Instance \texttt{e10}}
    \label{fig:stpg_pop.4}
    \end{subfigure}
    \caption[Effect of population size on number of partitions in the reduced sub-problem]{Effect of population size on number of partitions in the reduced sub-problem. Dotted line shows population size of 1,500, which was used for these experiments.} 
    \label{fig:stpg_pop_plots}
\end{figure*}
\end{center}
%
The second reason for choosing this population size is that in order for the MIP solver to be effective, the number of decision variables must be kept at a reasonable amount. It was decided that the number of partitions in the reduced sub-problem, after random splitting, should a maximum of around 2,000, on average. The number of random splits was set at 500, therefore the number of partitions before splitting should be around a maximum of 1,500 --- which occurs at a population size of around 1,500 for many of the problem instances. For larger problem instances which need a larger reduced sub-problem, an extra parameter was added \(M_{factor}\), which ensures that the reduced sub-problem is some fraction of the total instance size.
% \cmnt[angus]{should i go into more detail with the parameters? its already pretty long}

\section{Conclusion and future work}

Constrained optimisation problems involve the search for an optimal object in (often \emph{very} large) search spaces --- in the case of combinatorial optimisation, these spaces are finite and discrete. As well as this, problems that are abstractions of real-world processes can be extremely complex; having many additional side-constraints, separate to the main problem constraints themselves, which must be considered when modelling the problem mathematically.

\cite{np:karp} showed that these (\NP-Complete) problems are all theoretically reducible to one another; but in practice, they often require specific domain knowledge that means custom algorithms must be developed for each problem, individually. The discovery of a ``one size fits all'' solution for this whole class of problems would be a vital achievement in the field of operations research, and the purpose of this thesis was to attempt to provide a material step in the direction towards this goal.

Merge search, the algorithm proposed here, is a generalised hybrid meta-heuristic framework for solving large-scale and complex constrained optimisation problems --- with, or without specific domain knowledge. The operation of this framework can be broken down into several phases:
\begin{itemize}
\item \emph{Initial solution construction:} a feasible solution to the problem is constructed, either heuristically or by solving a generic mixed integer program (MIP) model of the problem that penalises non-feasible solutions;
\item \emph{Population generation:} a population of neighbouring solutions is generated, using some local search heuristic or by randomly perturbing an initial solution;
\item \emph{Partitioning variables:} the population of solutions is used to determine how the set of decision variables are partitioned. Additional splits to these partitions can be made to increase the granularity of the solutions produced; and,
\item \emph{Solving reduced sub-problem:} new local optima can be found by solving a reduced version of the problem which uses each partition as a meta-variable, either with a MIP solver or by some other method. By grouping the decision variables, this sub-problem requires much less computational effort to solve --- the trade-off being that the solution produced becomes an approximation of the optimal one.
\end{itemize}

A theoretical description of merge search was given along with a generic version which, although not as efficient as methods which utilise \emph{some} domain-specific knowledge, would serve to operate as a black-box solver for any combinatorial optimisation problem, provided a MIP model exists for it. It was also shown that merge search can be considered a generalisation of many optimisation techniques such as crossover and mutation operators in genetic algorithms, or search techniques like large neighbourhood search.

Practically, two problems were used as testing-grounds to demonstrate the effectiveness of merge search and explore some of its features and facets. The first was a complex, real-world problem from the field of open-pit mining called the constrained pit-limit (CPIT) problem and the second was a well-known problem from Karp's original 21 \NP-Complete problems, the Steiner tree problem in graphs (STPG). These two problems were chosen because they are very dissimilar and require very different approaches to solve them, which allowed the versatility and flexibility of this hybrid meta-heuristic framework to be showcased.

As it is a much simpler problem, the experiments on the STPG were used to investigate some of the main properties and design choices that need to be made when constructing merge search algorithms, specifically solution construction methods, population size, method of variable partitioning and methods of solving the reduced sub-problem. In contrast, the experiments on the CPIT problem highlighted the effectiveness of merge search in solving large scale, highly constrained, problems through its automatic decomposition aspects. They also investigated the effect that additional arbitrary splitting of the merge partitions has on the quality of the solutions produced.

While the datasets used for the STPG experiments already had been solved to optimality, and therefore were only being used to demonstrate the versatility of merge search and investigate its various aspects; the experiments on the CPIT problem produced solutions which improved on the upper-bounds of all six of the problem instances, as published on the \emph{minelib} website, and two of the most recent published upper-bounds as reported in~\cite{minelib-improved}.

\smallskip

Although demonstrating that merge search can be applied to two very different problems shows that the algorithm is very versatile, making the claim that it is a general framework that can be applied to \emph{any} combinatorial optimisation problem would require significantly more evidence than only two problems. Therefore, the most obvious future research that can be performed is in applying the technique to many more (and varying types of) problems. Aside from this, some further work could be done into developing a truly generic algorithm which operates entirely on the decision variables, and is completely problem agnostic; the interaction of different heuristics and other techniques, within the framework; the effectiveness of merge search with other exact solvers (such as constraint programming); the limits at which merge search is most effective; and, applying merge search to different classes (possibly non-discrete) of problems.


% \section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% % You must have at least 2 lines in the paragraph with the drop letter
% % (should never be an issue)
% I wish you the best of success.

% \hfill mds
 
% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.

% % needed in second column of first page if using \IEEEpubid
% %\IEEEpubidadjcol

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




% \section{Conclusion}
% The conclusion goes here.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


